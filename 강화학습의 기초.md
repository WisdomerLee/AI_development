## **강화 학습의 기초**

## 제 2장: 강화 학습의 기초

강화 학습은 에이전트가 환경과 상호작용하며 누적 보상을 최대화하는 최적의 행동 방식을 학습하는 과정입니다. 이 과정을 수학적으로 엄밀하게 정의하고 분석하기 위해, 강화 학습 분야에서는 **마르코프 결정 과정(Markov Decision Process, MDP)** 이라는 강력한 프레임워크를 사용합니다.[16, 17, 18, 19] MDP를 이해하는 것은 강화 학습 알고리즘의 작동 원리를 파악하는 데 필수적입니다.

### 2.1 에이전트-환경 상호작용 루프: 핵심 구성 요소

강화 학습 시스템은 몇 가지 핵심적인 구성 요소들의 상호작용으로 이루어집니다.[20]

*   **에이전트(Agent):** 학습의 주체이자 의사결정자입니다. 환경을 관찰하고 행동을 선택합니다.[11, 13, 18]
*   **환경(Environment):** 에이전트 외부의 모든 것을 의미하며, 에이전트의 행동에 반응하여 상태를 바꾸고 보상을 제공합니다.[11, 18, 20]
*   **상태(State, $s$):** 특정 시점에서의 환경에 대한 완전한 정보입니다. 에이전트는 이 상태를 기반으로 다음 행동을 결정합니다.[11, 18, 21]
*   **행동(Action, $a$):** 에이전트가 특정 상태에서 취할 수 있는 선택지입니다.[11, 18, 21]
*   **보상(Reward, $r$):** 에이전트가 특정 상태에서 특정 행동을 취한 결과로 환경으로부터 받는 스칼라 값의 피드백입니다. 보상은 그 행동이 단기적으로 얼마나 좋았는지를 나타내는 신호입니다.[11, 12, 21]

이 요소들은 다음과 같은 순환적인 상호작용 루프를 형성합니다 [12, 22]:
1.  시간 $t$에 에이전트는 환경의 상태 $s_t$를 관찰합니다.
2.  에이전트는 상태 $s_t$를 바탕으로 행동 $a_t$를 선택하여 수행합니다.
3.  환경은 에이전트의 행동 $a_t$에 따라 새로운 상태 $s_{t+1}$로 전이(transition)합니다.
4.  동시에 환경은 에이전트에게 즉각적인 보상 $r_{t+1}$을 제공합니다.
5.  에이전트는 새로운 상태 $s_{t+1}$과 보상 $r_{t+1}$을 받아들이고, 이 과정은 정해진 종료 조건이 만족될 때까지 반복됩니다.

### 2.2 문제의 공식화: 마르코프 결정 과정(MDP)

강화 학습에서 해결하고자 하는 순차적 의사결정 문제는 대부분 MDP를 통해 수학적으로 모델링될 수 있습니다.[16, 18, 19, 23] MDP는 '결과가 부분적으로는 무작위적이고 부분적으로는 의사결정자의 통제하에 있는' 상황을 모델링하는 프레임워크입니다.[17, 19]

#### 마르코프 속성(Markov Property)

MDP의 가장 근본적인 가정은 **마르코프 속성**입니다. 이는 "미래는 오직 현재에 의해서만 결정되며, 과거와는 무관하다"는 것을 의미합니다.[16, 24] 즉, 현재 상태 $s_t$가 주어지면, 다음 상태 $s_{t+1}$로 전이할 확률은 그 이전의 모든 상태들($s_0, s_1,..., s_{t-1}$)의 역사와는 독립적이라는 것입니다.[24] 이를 수식으로 표현하면 다음과 같습니다 [16, 24]:
$$\mathbb{P} = \mathbb{P}$$
이 속성 덕분에 에이전트는 과거의 모든 기록을 기억할 필요 없이 현재 상태 정보만으로 최적의 의사결정을 내릴 수 있으며, 이는 문제를 훨씬 다루기 쉽게 만듭니다.[25] 체스 게임에서 현재 기물의 배치만 보면 다음 수를 결정할 수 있는 상황은 마르코프 속성을 만족하는 좋은 예입니다.[25]

#### MDP의 5가지 구성요소 (Tuple)

MDP는 다음 5가지 요소의 튜플 $(S, A, P, R, \gamma)$로 정의됩니다 [23, 25, 26]:

*   $S$: 가능한 모든 상태의 유한 집합 (상태 공간, State Space).[21, 25]
*   $A$: 에이전트가 취할 수 있는 모든 행동의 유한 집합 (행동 공간, Action Space).[21, 25]
*   $P$: 상태 전이 확률 함수(State Transition Probability Function), $P(s'|s, a) = \mathbb{P}$. 상태 $s$에서 행동 $a$를 취했을 때 다음 상태 $s'$로 전이될 확률을 나타냅니다. 이는 환경의 동역학(dynamics)을 정의하며, 확률적일 수 있습니다.[16, 17, 25]
*   $R$: 보상 함수(Reward Function), $R(s, a, s') = \mathbb{E}$. 상태 $s$에서 행동 $a$를 취해 상태 $s'$로 전이했을 때 받을 것으로 기대되는 즉각적인 보상입니다.[25, 26, 27]
*   $\gamma$: **할인율(Discount Factor)**, $0 \le \gamma \le 1$. 미래 보상의 현재 가치를 결정하는 매우 중요한 하이퍼파라미터입니다.[16, 25, 27] $\gamma$가 0에 가까우면 에이전트는 즉각적인 보상만을 중시하는 "근시안적(myopic)"인 행동을 하게 되고, 1에 가까우면 미래의 보상까지 고려하는 "원시안적(farsighted)"인 행동을 하게 됩니다.[21]

현실 세계의 문제를 강화 학습으로 풀기 위해서는, 해당 문제를 이 MDP 프레임워크에 맞게 상태, 행동, 보상으로 정의하는 과정이 선행되어야 합니다. 예를 들어, 자율주행차 문제에서 '상태'를 단순히 차량의 위치와 속도로 정의할지, 아니면 주변 차량들의 정보, 신호등 상태, 날씨까지 포함할지는 중요한 설계 결정입니다.[28, 29] 만약 상태 정의가 불완전하여 미래 예측에 필요한 중요 정보(예: 숨겨진 장애물)를 누락한다면, 마르코프 속성이 깨지게 됩니다. 이러한 상황은 **부분 관측 마르코프 결정 과정(Partially Observable MDP, POMDP)**으로 알려져 있으며 [17], 이는 "모델 프리(model-free)" 알고리즘의 성공이 여전히 설계자의 암묵적인 세계 모델(상태 표현 방식)에 크게 의존함을 시사합니다.

### 2.3 최적성의 정량화: 정책과 가치 함수

MDP로 문제가 정의되면, 에이전트의 목표는 누적 보상을 최대화하는 최적의 행동 전략, 즉 **최적 정책($\pi^*$**)을 찾는 것입니다.[30]

*   **정책(Policy, $\pi$):** 에이전트의 행동 방식 또는 전략입니다. 특정 상태에서 어떤 행동을 선택할지를 결정하는 규칙으로, 주로 상태에 따른 행동의 확률 분포 $\pi(a|s) = \mathbb{P}$로 표현됩니다.[22, 25, 30, 31, 32]
*   **리턴(Return, $G_t$):** 특정 시점 $t$부터 에피소드가 끝날 때까지 받는 보상들의 할인된 총합입니다. 에이전트는 즉각적인 보상 $R_{t+1}$이 아니라, 리턴 $G_t$의 기댓값을 최대화하고자 합니다.[16, 21]
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
*   **가치 함수(Value Function):** 특정 정책 $\pi$를 따를 때, 어떤 상태 또는 상태-행동 쌍이 장기적으로 얼마나 좋은지를 나타내는 기댓값입니다. 대부분의 강화 학습 알고리즘의 핵심은 이 가치 함수를 추정하는 것입니다.[31, 32, 33, 34, 35]
    *   **상태-가치 함수(State-Value Function, $V^\pi(s)$):** 상태 $s$에서 시작하여 정책 $\pi$를 따랐을 때 기대되는 리턴입니다. "이 상태가 얼마나 좋은가?"에 대한 답을 줍니다.[16, 25, 30, 36]
        $$V^\pi(s) = \mathbb{E}_\pi$$
    *   **행동-가치 함수(Action-Value Function, $Q^\pi(s, a)$):** 상태 $s$에서 행동 $a$를 취한 후, 계속해서 정책 $\pi$를 따랐을 때 기대되는 리턴입니다. "이 상태에서 이 행동을 하는 것이 얼마나 좋은가?"에 대한 답을 주며, 흔히 **Q-함수(Q-function)**라고 불립니다.[25, 30, 33, 36]
        $$Q^\pi(s, a) = \mathbb{E}_\pi$$

할인율 $\gamma$는 단순히 수학적 수렴을 위한 장치가 아닙니다. 이는 에이전트의 '시간 선호도'와 '위험 회피 성향'을 조절하는 중요한 하이퍼파라미터입니다. 예를 들어, 금융 트레이딩 에이전트에게 낮은 $\gamma$를 설정하면 단기 수익을 위한 위험한 거래를 선호할 수 있고, 높은 $\gamma$를 설정하면 장기적 안정성을 추구하는 보수적인 거래를 할 수 있습니다.[37] 따라서 안전이 중요한 로보틱스나 자율주행 분야에서는 이론적 최적 수익률을 다소 희생하더라도 더 신중한 행동을 유도하기 위해 $\gamma$ 값을 조절하는 전략적 결정이 필요할 수 있습니다.[29, 38]

### 2.4 벨만 방정식: 해를 향한 재귀적 경로

가치 함수들은 현재 상태의 가치를 다음 상태의 가치와 즉각적인 보상으로 표현하는 재귀적 관계를 만족하는데, 이를 **벨만 방정식(Bellman Equation)**이라고 합니다.[35, 39]

*   **벨만 기대 방정식(Bellman Expectation Equation):** 주어진 정책 $\pi$에 대한 가치 함수를 계산합니다.
    *   $V^\pi(s) = \mathbb{E}_\pi = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma V^\pi(s')]$
    *   $Q^\pi(s, a) = \mathbb{E}_\pi = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]$

*   **벨만 최적 방정식(Bellman Optimality Equation):** 최적 정책 $\pi^*$에 대한 가치 함수, 즉 최적 가치 함수($V^*$와 $Q^*$)를 정의합니다. 최적 정책은 항상 가치를 최대로 만드는 행동을 선택하므로, 기댓값 연산에 `max` 연산자가 추가됩니다.
    *   $V^*(s) = \max_{a} \mathbb{E}$
    *   $Q^*(s, a) = \mathbb{E}$ [23, 40]

강화 학습 문제를 푼다는 것은 종종 벨만 최적 방정식을 만족하는 최적 가치 함수 $V^*$ 또는 $Q^*$를 찾는 과정과 동일합니다.[39] 이 방정식들은 동적 프로그래밍(Dynamic Programming), 시간차 학습(Temporal-Difference Learning) 등 수많은 강화 학습 알고리즘의 이론적 토대를 형성합니다.

---
