## **강화 학습의 핵심 알고리즘**

## 제 3장: 강화 학습의 핵심 알고리즘

강화 학습의 이론적 토대인 MDP와 벨만 방정식은 다양한 알고리즘을 통해 구체적인 해법으로 발전했습니다. 이 장에서는 강화 학습 분야에서 가장 중요하고 기초가 되는 알고리즘들인 Q-러닝, SARSA, DQN, 그리고 정책 경사법을 심도 있게 분석합니다. 각 알고리즘의 작동 원리, 수학적 수식, 그리고 고유한 특성을 비교하며 살펴보겠습니다.

### 3.1 Q-러닝: 최적 제어를 위한 오프폴리시 접근법

Q-러닝(Q-Learning)은 강화 학습에서 가장 널리 알려진 알고리즘 중 하나로, **모델-프리(model-free)**, **가치 기반(value-based)**, **오프폴리시(off-policy)**라는 세 가지 핵심 특징을 가집니다.[41, 42, 43] 모델-프리란 환경의 동역학(상태 전이 확률 $P$와 보상 함수 $R$)을 알지 못해도 학습이 가능하다는 의미이며, 가치 기반이란 최적 행동-가치 함수($Q^*$)를 학습하여 간접적으로 최적 정책을 찾는다는 의미입니다.

#### Q-테이블(Q-Table)

상태와 행동 공간이 이산적이고 충분히 작은 문제의 경우, Q-함수는 간단한 표(table) 형태로 구현될 수 있습니다. 이 표를 **Q-테이블**이라 하며, 행은 상태(state)를, 열은 행동(action)을 나타냅니다. 테이블의 각 셀에는 해당 상태-행동 쌍의 Q-값, 즉 $Q(s, a)$가 저장됩니다. Q-러닝 알고리즘의 목표는 이 테이블을 최적의 Q-값($Q^*$)으로 채워나가는 것입니다.[41, 42, 43, 44, 45]

#### Q-러닝 업데이트 규칙

Q-러닝의 핵심은 **시간차(Temporal Difference, TD) 업데이트** 규칙에 있습니다. 에이전트가 상태 $s$에서 행동 $a$를 취하고, 보상 $r$을 받으며 다음 상태 $s'$를 관찰했을 때, $Q(s, a)$ 값은 다음과 같이 업데이트됩니다 [41, 46]:

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

이 수식을 구성 요소별로 분석하면 다음과 같습니다:
*   $Q(s, a)$: 업데이트 전의 현재 추정치 (Old Value).
*   $\alpha$: **학습률(Learning Rate)**. 새로운 정보(TD 타겟)를 얼마나 반영할지를 결정하는 0과 1 사이의 값입니다.[41, 44]
*   $r + \gamma \max_{a'} Q(s', a')$: **TD 타겟(TD Target)**. 벨만 최적 방정식에 기반한 $Q(s, a)$의 새로운 목표값입니다. 이는 즉각적인 보상 $r$과 다음 상태 $s'$에서 취할 수 있는 *가장 가치 있는 행동*의 할인된 가치를 더한 것입니다.
*   $[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$: **TD 오차(TD Error)**. 목표값과 현재 추정치 사이의 차이로, "놀람(surprise)"의 정도를 나타냅니다.[47]

#### 오프폴리시(Off-Policy) 특성

Q-러닝이 오프폴리시인 이유는 업데이트 규칙의 `$\max_{a'} Q(s', a')$` 항 때문입니다. 이 항은 다음 상태 $s'$에서 실제로 어떤 행동을 선택할지와 무관하게, 항상 Q-값이 가장 큰 행동(즉, 탐욕적(greedy) 정책에 따른 행동)을 가정하여 현재의 Q-값을 업데이트합니다. 이로 인해 에이전트가 데이터를 수집하기 위해 따르는 **행동 정책(behavior policy)**(예: 탐험을 위해 무작위 행동을 섞는 $\epsilon$-greedy 정책)과, 학습의 목표가 되는 **타겟 정책(target policy)**(탐욕적 정책)이 분리됩니다. 즉, 다른 정책을 따르면서도 최적 정책을 학습할 수 있는 것입니다.[46, 48, 49, 50, 51, 52, 53]

### 3.2 SARSA: 온폴리시 대응 알고리즘

SARSA는 Q-러닝과 매우 유사하지만, 결정적인 차이점을 가진 **온폴리시(on-policy)** 알고리즘입니다.[48, 49, 54, 55] SARSA라는 이름은 업데이트에 사용되는 데이터의 순서, 즉 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ 튜플에서 유래했습니다.

#### SARSA 업데이트 규칙

SARSA의 업데이트 규칙은 다음과 같습니다 [48, 56]:

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)]$$

Q-러닝과 비교했을 때, TD 타겟이 `r + \gamma Q(s', a')`로 변경된 것을 볼 수 있습니다.

#### 온폴리시(On-Policy) 특성

가장 큰 차이점은 `max` 연산자가 없다는 것입니다. 여기서 $a'$는 다음 상태 $s'$에서 **실제로 선택한 행동**입니다. 이 행동은 현재 에이전트가 따르고 있는 행동 정책(예: $\epsilon$-greedy)에 의해 결정됩니다. 따라서 SARSA는 탐험을 포함하여 에이전트가 실제로 따르고 있는 정책의 가치를 학습합니다. 즉, 행동 정책과 타겟 정책이 동일하기 때문에 온폴리시 알고리즘으로 분류됩니다.[48, 50, 53, 57, 58, 59]

### 3.3 비교 분석: 온폴리시 대 오프폴리시 학습

온폴리시와 오프폴리시의 구분은 강화 학습 알고리즘을 이해하는 핵심적인 축이며, 학습 안정성과 실제 적용 시 안전성에 직접적인 영향을 미칩니다.

*   **핵심 차이:** 온폴리시(SARSA)는 현재 실행 중인 정책의 가치를 평가하고 개선합니다(행동 정책 = 타겟 정책). 반면, 오프폴리시(Q-러닝)는 행동 정책과 무관하게 최적 정책의 가치를 학습합니다(행동 정책 ≠ 타겟 정책).[53, 60, 61, 62, 63, 64, 65, 66]
*   **샘플 효율성:** 오프폴리시 방법은 일반적으로 샘플 효율성이 더 높습니다. 과거 정책에서 수집된 오래된 데이터까지 학습에 재사용할 수 있기 때문입니다(이는 DQN의 경험 리플레이의 핵심 아이디어입니다). 반면, 온폴리시 방법은 정책이 업데이트될 때마다 이전 데이터를 폐기해야 하므로 더 많은 환경 상호작용이 필요합니다.[61, 62, 63]
*   **절벽 보행(Cliff Walking) 문제:** 이 고전적인 예제는 두 알고리즘의 행동 차이를 극명하게 보여줍니다.[67, 68, 69, 70]
    *   **상황:** 에이전트는 출발점에서 도착점까지 가야 하며, 절벽으로 떨어지면 큰 음의 보상을 받습니다. 최단 경로는 절벽 바로 옆을 따라가는 길이고, 더 멀리 돌아가는 안전한 길이 있습니다.
    *   **Q-러닝의 행동:** Q-러닝은 최적 정책을 학습하므로 절벽 가장자리를 따라가는 최단 경로를 학습합니다. `max` 연산자로 인해 탐험($\epsilon$ 확률)으로 인해 절벽에 빠질 수 있는 위험을 고려하지 않고 낙관적으로 가치를 평가합니다. 따라서 훈련 중에는 자주 절벽에 떨어져 낮은 보상을 받지만, 학습이 완료되면 가장 효율적인 경로를 알게 됩니다.[68, 69]
    *   **SARSA의 행동:** SARSA는 자신의 $\epsilon$-greedy 정책의 가치를 학습합니다. 따라서 절벽 근처에서 무작위 행동을 할 경우 큰 페널티를 받을 수 있다는 것을 학습 과정에 반영합니다. 결과적으로 절벽을 피해 돌아가는 더 길지만 안전한 경로를 학습하는 경향이 있습니다. 즉, 더 "보수적"입니다.[68, 71, 72]

이러한 차이는 실제 적용에서 중요한 의미를 가집니다. 실물 로봇처럼 실수가 비용이 많이 들거나 위험한 환경에서는 학습 과정에서의 안전을 위해 보수적인 온폴리시 알고리즘(SARSA)이 선호될 수 있습니다.[50, 58, 69] 반면, 실수가 비용이 들지 않는 시뮬레이션 환경(예: 게임)에서는 더 공격적으로 탐험하고 샘플 효율성이 높은 오프폴리시 알고리즘(Q-러닝)이 더 빠르게 최적 해를 찾는 데 유리합니다.[50, 58, 70]

| 특성 | Q-러닝 (Q-Learning) | SARSA |
| :--- | :--- | :--- |
| **정책 유형** | 오프폴리시 (Off-policy) [46, 50] | 온폴리시 (On-policy) [50, 55] |
| **업데이트 규칙** | $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$ | $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)]$ |
| **업데이트 시 행동 선택** | 다음 상태에서 가능한 모든 행동 중 Q-값이 최대인 행동을 가정 [50, 73] | 다음 상태에서 현재 정책($\epsilon$-greedy)에 따라 실제로 선택된 행동 사용 [50, 73] |
| **탐험의 영향** | 업데이트 시 탐험적 행동의 위험을 무시하고 낙관적으로 평가 [68] | 업데이트 시 탐험적 행동의 위험을 가치 평가에 반영 [68] |
| **수렴 목표** | 최적(탐욕적) 정책 $Q^*$로 수렴 [68] | 현재 따르는($\epsilon$-greedy) 정책의 $Q^\pi$로 수렴 [68] |
| **대표적 행동 양상** | 절벽 보행 문제에서 위험하지만 최단 경로 선호 (공격적) [68, 71] | 절벽 보행 문제에서 돌아가더라도 안전한 경로 선호 (보수적) [68, 71] |
| **안정성** | Q-값 과대평가(overestimation) 경향이 있을 수 있음 [50] | Q-값 과대평가 경향이 적어 더 안정적일 수 있음 [50] |
| **샘플 효율성** | 일반적으로 더 높음 (경험 재사용 가능) [61, 62] | 일반적으로 더 낮음 (정책 변경 시 이전 샘플 폐기) [61, 62] |

### 3.4 심층 Q-네트워크(DQN): 고차원 공간으로의 확장

Q-테이블과 같은 테이블 기반 방법은 상태 공간이 매우 크거나 연속적일 때 **차원의 저주(curse of dimensionality)** 문제에 직면하여 현실적으로 적용이 불가능합니다.[40, 41] 예를 들어, 비디오 게임 화면(픽셀 값)을 상태로 사용하는 경우, 가능한 상태의 수는 거의 무한대에 가깝습니다.

이 문제를 해결하기 위해 **함수 근사(function approximation)** 기법이 도입되었습니다. 심층 Q-네트워크(Deep Q-Network, DQN)는 심층 신경망(Deep Neural Network)을 사용하여 Q-함수를 근사하는 획기적인 방법입니다: $Q(s, a; \theta) \approx Q^*(s, a)$. 여기서 $\theta$는 신경망의 가중치(파라미터)를 의미합니다.[74, 75, 76] 이로써 심층 강화 학습(Deep Reinforcement Learning) 시대가 열렸습니다.

#### 안정적인 학습을 위한 핵심 혁신

단순히 Q-러닝과 신경망을 결합하는 것은 학습이 매우 불안정합니다.[76] 2015년 딥마인드(DeepMind)는 아타리(Atari) 게임에서 인간 수준을 뛰어넘는 성능을 보인 DQN을 발표하며, 이 불안정성을 해결하기 위한 두 가지 핵심 기법을 제시했습니다 [75]:

1.  **경험 리플레이(Experience Replay):** 에이전트의 경험, 즉 상태 전이 튜플 $(s, a, r, s')$을 **리플레이 버퍼(replay buffer)**라는 메모리에 순차적으로 저장합니다.[40, 74, 77] 학습 시에는 이 버퍼에서 무작위로 미니배치(mini-batch)를 샘플링하여 신경망을 업데이트합니다. 이 방식은 연속된 샘플 간의 강한 상관관계를 깨뜨려 학습을 안정시키고, 한 번의 경험을 여러 번 재사용하여 데이터 효율성을 크게 높입니다.[40, 75, 77]
2.  **고정된 Q-타겟(Fixed Q-Targets):** 두 개의 신경망을 사용합니다. 하나는 매 스텝 업데이트되는 **정책 네트워크(policy network, $\theta$)**이고, 다른 하나는 TD 타겟을 계산하는 데 사용되는 **타겟 네트워크(target network, $\theta^-$)**입니다.[75, 78, 79] 타겟 네트워크의 가중치는 일정 기간 동안 고정되어 있다가, 주기적으로 정책 네트워크의 가중치로 복사됩니다. 이는 손실 함수의 타겟값을 안정시켜, "자신의 꼬리를 쫓는 개"처럼 타겟이 매 스텝 흔들리는 문제를 방지하고 학습을 안정화시킵니다.[75, 78]

#### DQN 아키텍처 및 손실 함수

*   **아키텍처:** 입력이 이미지일 경우 CNN(합성곱 신경망)을, 벡터 형태일 경우 MLP(다층 퍼셉트론)를 주로 사용합니다.[28, 75] 네트워크는 상태를 입력으로 받아 가능한 모든 행동에 대한 Q-값 벡터를 출력합니다.
*   **손실 함수:** TD 타겟과 정책 네트워크의 예측값 사이의 평균 제곱 오차(Mean Squared Error, MSE) 또는 이상치에 더 강건한 후버 손실(Huber Loss)을 사용합니다.[40, 78]
    $$ \text{Loss}(\theta) = \mathbb{E}_{(s, a, r, s') \sim \text{Buffer}} \left[ \left( y_j - Q(s_j, a_j; \theta) \right)^2 \right] $$
    여기서 타겟 $y_j = r_j + \gamma \max_{a'} Q(s'_j, a'; \theta^-)$는 **타겟 네트워크**를 사용하여 계산됩니다.

### 3.5 정책 경사법: 정책으로 가는 직접적인 경로

가치 함수를 학습하고 그로부터 정책을 유도하는 대신, 정책 자체를 직접 학습하는 방법도 있습니다. 이를 **정책 기반(policy-based)** 방법이라 하며, **정책 경사법(Policy Gradient Methods)**이 대표적입니다.[80] 이 접근법은 로봇 제어와 같이 행동 공간이 연속적인 경우(가치 기반 방법에서 `argmax` 연산이 어려운 경우)나, 가위바위보처럼 최적 정책이 확률적인 경우에 특히 유용합니다.[80, 81]

#### 핵심 아이디어

정책을 파라미터 $\theta$를 가진 함수 $\pi(a|s, \theta)$로 직접 모델링합니다. 그리고 정책의 성능을 나타내는 목적 함수 $J(\theta)$(예: 시작 상태의 기대 리턴)를 정의한 후, 이 목적 함수의 경사(gradient)를 따라 파라미터 $\theta$를 업데이트(경사 상승법)하여 정책을 개선합니다 [82, 83]:
$$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$$

#### 정책 경사 정리(Policy Gradient Theorem)

환경의 동역학을 몰라도 목적 함수의 경사를 계산할 수 있는 방법을 제공하는 중요한 정리입니다. 경사의 한 형태는 다음과 같습니다 [83, 84]:
$$\nabla_\theta J(\theta) = \mathbb{E}_\pi$$
*   $\nabla_\theta \log \pi(A_t|S_t, \theta)$: **스코어 함수(score function)**라고 불립니다.
*   $G_t$: 시점 $t$에서의 리턴(누적 보상)입니다. 이 항은 파라미터 업데이트의 방향과 크기를 결정합니다. 만약 리턴 $G_t$가 높았다면, 그 리턴을 이끈 행동 $A_t$의 확률을 높이는 방향으로 $\theta$를 업데이트하고, 리턴이 낮았다면 그 반대로 업데이트합니다.

#### REINFORCE 알고리즘

REINFORCE는 몬테카를로 정책 경사법의 기초가 되는 알고리즘입니다.[80, 83] 하나의 완전한 에피소드를 실행하여 각 시점의 리턴 $G_t$를 계산한 뒤, 이 값을 이용해 정책 경사 정리에 따라 정책 파라미터를 업데이트합니다.[81, 84]

#### 높은 분산 문제와 해결책

REINFORCE의 주요 단점은 리턴 $G_t$가 에피소드마다 크게 달라질 수 있어 경사 추정치의 분산(variance)이 매우 높다는 것입니다.[80, 84] 이는 학습을 불안정하게 만듭니다. 이 문제를 완화하기 위해 리턴에서 **베이스라인(baseline)** $b(s)$을 빼주는 기법을 사용합니다. 베이스라인은 행동 $a$에 의존하지 않아야 하며, 주로 상태-가치 함수 $V(s)$가 사용됩니다. 이는 **어드밴티지 함수(Advantage Function)** $A(s, a) = Q(s, a) - V(s)$의 개념으로 이어집니다. 즉, 단순히 리턴이 높고 낮음이 아니라, 그 상태에서의 '평균적인' 가치보다 얼마나 더 좋았는지를 기준으로 학습하는 것입니다.[80, 83] 이는 **액터-크리틱(Actor-Critic)** 방법의 이론적 토대가 됩니다.

이러한 알고리즘들의 발전은 단순한 선형적 개선이 아니라, 각기 다른 문제 유형과 환경에 대응하기 위한 RL 툴킷의 확장으로 보아야 합니다. Q-러닝에서 DQN으로의 발전이 상태 공간의 한계를 극복했다면, 정책 경사법은 행동 공간의 한계(연속성)를 극복했습니다. 결국 가치 기반 방법(DQN)과 정책 기반 방법(정책 경사법)의 근본적인 장단점(예: 샘플 효율성 대 적용 일반성)은 두 가지를 결합한 액터-크리틱(Actor-Critic) 계열 알고리즘(A2C, A3C, SAC 등)의 등장을 촉발했습니다. 이는 알고리즘 설계의 하이브리드화라는 뚜렷한 경향을 보여줍니다.[20, 81, 85, 86]

---
