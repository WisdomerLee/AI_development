## **구현: 이론에서 코드로**

## 제 4장: 구현: 이론에서 코드로

이론적 개념과 알고리즘을 실제 코드로 구현하는 과정에는 여러 실용적인 고려사항이 따릅니다. 이 장에서는 강화 학습 알고리즘을 성공적으로 구현하기 위해 반드시 다루어야 할 핵심 주제들—탐험과 활용의 딜레마, 보상 함수 설계, 하이퍼파라미터 튜닝—을 먼저 논의하고, 이어서 파이썬과 표준 라이브러리를 사용한 구체적인 코드 구현 예제를 단계별로 살펴보겠습니다.

### 4.1 실용적 구현을 위한 필수 고려사항

알고리즘을 코드화하기 전에, 학습의 효율성과 성공 여부를 결정짓는 몇 가지 중요한 설계 결정이 필요합니다.

#### 탐험과 활용의 딜레마(Exploration-Exploitation Dilemma)

강화 학습에서 에이전트는 두 가지 상충하는 목표 사이에서 균형을 잡아야 합니다.[87, 88]
*   **활용(Exploitation):** 현재까지의 지식을 바탕으로 가장 높은 보상을 줄 것으로 기대되는 행동을 선택하는 것.
*   **탐험(Exploration):** 더 나은 보상을 가져다줄지도 모르는 새로운 행동을 시도하여 환경에 대한 더 많은 정보를 수집하는 것.

지나친 활용은 차선책에 머무르게 할 수 있고, 지나친 탐험은 이미 알고 있는 좋은 행동을 통해 얻을 수 있는 보상을 놓치게 만듭니다.[87, 89] 이 딜레마는 지도 학습이나 비지도 학습에서는 나타나지 않는 강화 학습 고유의 문제입니다.[90]

*   **$\epsilon$-탐욕($\epsilon$-Greedy) 정책:** 이 딜레마를 해결하는 가장 간단하고 일반적인 전략입니다. 에이전트는 $1-\epsilon$의 확률로 현재 가장 Q-값이 높은 행동을 선택(활용)하고, $\epsilon$의 확률로 무작위 행동을 선택(탐험)합니다.[41, 48, 87]
*   **감쇠하는 엡실론(Decaying Epsilon):** 학습 초기에는 높은 $\epsilon$ 값으로 시작하여 탐험을 장려하고, 학습이 진행됨에 따라 점차 $\epsilon$ 값을 줄여 활용의 비중을 높이는 것이 일반적인 접근법입니다.[79, 90, 91]
*   **고급 전략:** 더 정교한 방법으로는 각 행동의 불확실성을 고려하는 UCB(Upper Confidence Bound)나 베이즈 정리를 활용하는 톰슨 샘플링(Thompson Sampling) 등이 있습니다.[87, 89, 92]

#### 보상 함수 설계: 보상 설계(Reward Shaping)의 기술과 과학

많은 실제 문제에서 보상은 매우 **희소(sparse)**합니다. 예를 들어, 체스 게임에서는 게임이 끝날 때만 승패에 따른 보상(+1 또는 -1)이 주어집니다. 이러한 환경에서 에이전트는 어떤 행동이 최종 결과에 기여했는지 파악하기 매우 어렵습니다.[93, 94, 95]

*   **보상 설계(Reward Shaping):** 이 문제를 해결하기 위해, 환경의 기본 보상에 추가적인 중간 보상을 설계하여 에이전트의 학습을 유도하고 수렴 속도를 높이는 기법입니다.[93, 96, 97, 98] 예를 들어, 미로 찾기 문제에서 출구에 도달했을 때만 보상을 주는 대신, 출구에 가까워질 때마다 작은 양의 보상을 주는 방식입니다.[93, 94]
*   **잠재력 기반 보상 설계(Potential-Based Reward Shaping):** 보상 설계를 잘못하면 에이전트가 원래 목표 대신 설계된 중간 보상을 얻는 데만 집중하는 **보상 해킹(reward hacking)** 문제가 발생할 수 있습니다.[93, 94, 95] 이를 방지하기 위해, 추가 보상을 상태의 '잠재력(potential)' 함수의 차이로 정의하는 방법이 제안되었습니다. 이 방법은 최적 정책이 변하지 않음을 이론적으로 보장하면서도 학습을 가속화할 수 있습니다.[93, 98]
*   **주의점:** 보상 함수 설계는 도메인 지식이 크게 요구되는 섬세한 작업이며, 잘못된 설계는 예기치 않은 부작용을 낳을 수 있습니다. 따라서 간단한 보상 구조에서 시작하여 점진적으로 실험하고 수정하는 반복적인 접근이 권장됩니다.[94, 97]

#### 하이퍼파라미터 튜닝: 강화 학습을 위한 모범 사례

강화 학습 알고리즘은 하이퍼파라미터 설정에 매우 민감하며, 기본값으로 좋은 성능을 기대하기는 어렵습니다.[99, 100, 101, 102]

*   **주요 하이퍼파라미터:**
    *   `학습률 (α)`: 업데이트 단계의 크기를 조절합니다.
    *   `할인율 (γ)`: 현재 보상과 미래 보상 간의 중요도를 조절합니다.
    *   `탐험률 (ε)`: 탐험과 활용의 균형을 맞춥니다.
    *   `배치 크기 (batch_size)`: DQN에서 리플레이 버퍼로부터 샘플링하는 경험의 수입니다.
    *   `네트워크 아키텍처`: 신경망의 층 수, 뉴런 수, 활성화 함수 등입니다.
*   **튜닝 방법:**
    *   **수동 튜닝(Manual Tuning):** 직관과 경험에 의존한 시행착오 방식입니다.[103]
    *   **그리드 탐색(Grid Search):** 미리 정의된 하이퍼파라미터 값들의 모든 조합을 시도합니다. 계산 비용이 매우 높습니다.[103, 104]
    *   **랜덤 탐색(Random Search):** 정의된 탐색 공간에서 무작위로 조합을 샘플링합니다. 종종 그리드 탐색보다 효율적입니다.[103, 104]
    *   **베이즈 최적화 및 자동화된 HPO:** 확률적 모델을 사용하여 더 유망한 하이퍼파라미터 영역을 지능적으로 탐색하는 고급 기법입니다. Optuna와 같은 라이브러리나 Stable-Baselines3-zoo와 같은 프레임워크에서 널리 사용됩니다.[99, 101, 104]
*   **모범 사례:** 강화 학습은 무작위성에 크게 영향을 받으므로, 항상 여러 개의 다른 랜덤 시드(seed)로 여러 번 실행하여 성능을 통계적으로 평가해야 합니다. 또한, 튜닝에 사용된 시드와 최종 성능 평가에 사용된 시드를 분리하여 하이퍼파라미터가 특정 시드에 과적합되는 것을 방지해야 합니다.[99, 100, 101]

### 4.2 코드 구현: 실용적인 단계별 가이드

이제 이론을 실제 코드로 옮겨보겠습니다. 강화 학습 알고리즘 개발 및 비교를 위한 표준 도구인 **Gymnasium**(구 OpenAI Gym)을 사용합니다.[105, 106, 107]

#### 환경 설정: Gymnasium

Gymnasium은 에이전트가 상호작용할 수 있는 다양한 환경을 제공하는 라이브러리입니다. 핵심 API는 다음과 같습니다.
*   `gym.make("Environment-Name")`: 특정 환경을 생성합니다.
*   `env.reset()`: 환경을 초기 상태로 되돌리고, 초기 관측값을 반환합니다.
*   `env.step(action)`: 주어진 행동을 수행하고, `(observation, reward, terminated, truncated, info)` 튜플을 반환합니다.
*   `env.render()`: 환경의 현재 상태를 시각화합니다.

#### 예제 1: `FrozenLake-v1` 환경에서 테이블형 Q-러닝 구현 (Python)

이 예제는 테이블 기반 Q-러닝 알고리즘을 처음부터 구현하는 과정을 보여줍니다. `FrozenLake-v1`은 작은 격자 세계 환경으로, 테이블 방식에 적합합니다.[105, 108, 109, 110]

**코드 구조 [91, 109, 110]:**

1.  **라이브러리 임포트:** `numpy`, `gymnasium`, `random`을 임포트합니다.
2.  **환경 초기화:** `gym.make("FrozenLake-v1")`로 환경을 생성하고, `env.observation_space.n`과 `env.action_space.n`을 통해 상태와 행동 공간의 크기를 얻습니다.
3.  **Q-테이블 초기화:** `np.zeros((state_space_size, action_space_size))`를 사용하여 모든 값을 0으로 채운 Q-테이블을 생성합니다.
4.  **하이퍼파라미터 정의:** `learning_rate`, `gamma`, `epsilon`, `num_episodes` 등 학습에 필요한 파라미터들을 설정합니다.
5.  **메인 학습 루프 구현:**
    *   에피소드 수만큼 반복하는 외부 루프를 만듭니다.
    *   각 에피소드 시작 시 `env.reset()`으로 환경을 초기화합니다.
    *   에피소드가 끝날 때까지 반복하는 내부 루프를 만듭니다.
    *   **$\epsilon$-greedy 정책 구현:** `random.uniform(0, 1)`으로 난수를 생성하여 `epsilon`과 비교합니다. 난수가 `epsilon`보다 크면 `np.argmax(q_table[state, :])`로 가장 큰 Q-값을 가진 행동을 선택(활용)하고, 그렇지 않으면 `env.action_space.sample()`로 무작위 행동을 선택(탐험)합니다.
    *   `env.step(action)`으로 선택된 행동을 수행하고, `new_state`, `reward`, `done` 정보를 받습니다.
    *   **Q-러닝 업데이트 규칙 적용:** `q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])` 수식을 코드로 구현합니다.
    *   `state = new_state`로 현재 상태를 업데이트하고, `done`이 `True`이면 내부 루프를 종료합니다.
6.  **평가 및 시각화:** 학습이 끝난 후, 탐험을 끄고(`epsilon=0`) 학습된 Q-테이블을 사용하여 에이전트의 성능을 평가하고, `env.render()`를 통해 행동을 시각화합니다.

#### 예제 2: `CartPole-v1` 환경에서 DQN 구현 (PyTorch)

이 예제는 연속적인 상태 공간을 가진 문제를 해결하기 위해 테이블 방식에서 심층 신경망으로 확장하는 방법을 보여줍니다. `CartPole-v1`은 카트 위의 막대가 쓰러지지 않도록 제어하는 고전적인 제어 문제입니다.[79, 106, 111, 112]

**코드 구조 [79, 111, 112, 113]:**

1.  **라이브러리 임포트:** `gymnasium`, `torch`, `torch.nn`, `torch.optim`, `collections`, `random` 등을 임포트합니다.
2.  **`ReplayMemory` 클래스 정의:** `collections.deque`를 사용하여 고정된 크기의 버퍼를 만듭니다. `push` 메서드로 경험 튜플 `(state, action, next_state, reward)`을 저장하고, `sample` 메서드로 버퍼에서 지정된 배치 크기만큼 무작위로 경험을 추출합니다.
3.  **`DQN` 클래스 정의:** `torch.nn.Module`을 상속받는 신경망 클래스를 정의합니다. `CartPole`의 상태는 4차원 벡터이므로, 입력층의 크기는 4, 행동은 '왼쪽'과 '오른쪽' 두 가지이므로 출력층의 크기는 2인 간단한 MLP(다층 퍼셉트론)를 구성합니다.
4.  **하이퍼파라미터 및 설정:** `BATCH_SIZE`, `GAMMA`(할인율), `EPS_START`, `EPS_END`, `EPS_DECAY`(엡실론 감쇠 스케줄), `TAU`(타겟 네트워크 업데이트 속도), `LR`(학습률) 등을 설정합니다. `policy_net`과 `target_net`을 `DQN` 클래스로부터 인스턴스화하고, `target_net`의 가중치를 `policy_net`과 동일하게 초기화합니다. `AdamW`와 같은 옵티마이저를 설정합니다.
5.  **`select_action` 함수 구현:** $\epsilon$-greedy 로직을 구현합니다. 엡실론 값은 에피소드가 진행됨에 따라 지수적으로 감쇠시킵니다.
6.  **`optimize_model` 함수 구현 (학습의 핵심):**
    *   리플레이 메모리에서 `sample` 메서드를 호출하여 미니배치를 가져옵니다.
    *   배치로부터 `state`, `action`, `reward`, `next_state` 텐서를 분리합니다.
    *   **정책 네트워크**를 사용하여 현재 상태에 대한 Q-값(`state_action_values`)을 계산합니다.
    *   **타겟 네트워크**를 사용하여 다음 상태에 대한 최대 Q-값(`next_state_values`)을 계산합니다. 에피소드가 종료된 상태에 대해서는 가치를 0으로 처리합니다.
    *   TD 타겟(`expected_state_action_values`)을 `(next_state_values * GAMMA) + reward_batch` 공식으로 계산합니다.
    *   `Huber loss`나 `Smooth L1 loss`와 같은 손실 함수를 사용하여 `state_action_values`와 `expected_state_action_values` 간의 오차를 계산합니다.
    *   `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`을 통해 정책 네트워크의 가중치를 업데이트합니다.
7.  **메인 학습 루프:**
    *   에피소드 수만큼 반복합니다.
    *   각 스텝에서 `select_action`으로 행동을 선택하고, `env.step`으로 환경과 상호작용하여 경험을 얻고, 이를 리플레이 메모리에 `push`합니다.
    *   `optimize_model` 함수를 호출하여 네트워크를 학습시킵니다.
    *   일정 스텝마다(`TAU` 사용) `target_net`의 가중치를 `policy_net`의 가중치로 업데이트합니다.
8.  **결과 시각화:** 에피소드별 지속 시간(보상)을 기록하고 그래프로 그려 학습 과정을 모니터링합니다.

단순한 테이블 기반 Q-러닝에서 DQN으로의 전환은 상당한 엔지니어링 복잡성을 동반합니다. 리플레이 메모리, 타겟 네트워크, 엡실론 스케줄링과 같은 추가 구성 요소들은 단순히 신경망을 추가하는 것을 넘어, 강력하지만 불안정한 함수 근사기를 사용할 때 학습 과정을 안정시키기 위한 필수적인 장치들입니다.[75, 76, 79] 이는 성공적인 심층 강화 학습 전문가가 알고리즘 이론가일 뿐만 아니라, 이러한 복잡한 상호작용 시스템을 구축하고 디버깅하는 능숙한 엔지니어여야 함을 시사합니다. 이러한 복잡성 때문에 Stable-Baselines3와 같은 잘 구축된 라이브러리가 연구와 개발에서 널리 사용되는 것입니다.[99]

---
