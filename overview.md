## **인공지능 학습의 지형도**

# 강화 학습 연구자 대계: 기초 이론부터 실제 구현까지

## 제 1장: 인공지능 학습의 지형도

인공지능(AI) 기술의 발전은 컴퓨터가 데이터를 통해 학습하고, 논리적으로 사고하며, 패턴을 인식하여 예측을 수행하는 능력에 기반합니다.[1] 이러한 학습 능력은 문제의 종류와 가용한 데이터의 형태에 따라 여러 패러다임으로 분화하며, 각 패러다임은 고유한 철학과 방법론을 가집니다. 가장 대표적인 세 가지 패러다임은 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 그리고 강화 학습(Reinforcement Learning)입니다.[1, 2, 3] 이들을 이해하는 것은 강화 학습의 독특한 위치와 역할을 파악하기 위한 필수적인 첫걸음입니다.

### 1.1 지도 학습: 정답이 있는 데이터로부터의 학습

지도 학습(Supervised Learning)은 가장 직관적이고 널리 사용되는 머신러닝 패러다임으로, '정답'이 명시된 데이터셋을 기반으로 학습을 진행합니다.[2, 4] 시스템은 입력 변수($X$)와 그에 해당하는 결과 변수($Y$, 즉 레이블 또는 정답) 사이의 관계를 나타내는 매핑 함수를 학습하는 것을 목표로 합니다. 이 학습된 함수를 통해 새로운, 보지 못했던 입력 데이터에 대한 결과를 정확하게 예측할 수 있게 됩니다.[2]

#### 주요 과제

*   **분류(Classification):** 주어진 데이터를 미리 정의된 여러 범주 중 하나로 할당하는 문제입니다.[2, 5] 예를 들어, 이메일 내용을 보고 스팸인지 아닌지를 판별하는 이진 분류(binary classification)나, 이미지를 보고 사과, 바나나, 포도 등으로 구분하는 다중 분류(multi-class classification)가 여기에 해당합니다.[2, 6]
*   **회귀(Regression):** 데이터의 특징(feature)을 기반으로 연속적인 값을 예측하는 문제입니다.[2, 5] 주택의 크기, 위치, 건축 연도 등의 데이터를 바탕으로 주택 가격을 예측하거나, 과거 판매 데이터를 기반으로 미래의 매출을 예측하는 것이 대표적인 회귀 과제입니다.[5, 6]

#### 근본적 요구사항

지도 학습의 성능은 학습 데이터의 양과 질에 절대적으로 의존합니다.[7] 정확하고 편향되지 않은 대규모의 레이블링된 데이터를 구축하는 과정은 많은 시간과 비용, 그리고 해당 분야의 전문 지식을 요구하는 병목 구간이 될 수 있습니다.[5, 7]

### 1.2 비지도 학습: 숨겨진 구조의 발견

비지도 학습(Unsupervised Learning)은 지도 학습과 달리 레이블이나 정답이 없는 데이터를 대상으로 합니다.[2, 7, 8] 이 패러다임의 목표는 데이터 내에 잠재적으로 존재하는 패턴, 구조, 또는 그룹을 스스로 발견하는 것입니다.[5, 7] 정답이 없기 때문에, 데이터의 본질적인 특성을 이해하거나 유용한 형태로 요약하는 데 중점을 둡니다.[7]

#### 주요 과제

*   **군집화(Clustering):** 유사한 특성을 가진 데이터 포인트들을 하나의 그룹(클러스터)으로 묶는 작업입니다.[5, 6] 예를 들어, 고객의 구매 패턴을 분석하여 여러 고객 세그먼트로 나누고, 각 세그먼트에 맞는 마케팅 전략을 수립하는 데 활용될 수 있습니다.[6, 9]
*   **연관 규칙 학습(Association):** 데이터셋 내 변수들 간의 흥미로운 관계를 찾는 것을 목표로 합니다.[5] "기저귀를 구매한 고객은 맥주도 함께 구매하는 경향이 있다"와 같은 규칙을 발견하는 장바구니 분석이 대표적인 예입니다.
*   **차원 축소(Dimensionality Reduction):** 데이터의 무결성을 최대한 유지하면서 특성(feature)의 개수를 줄이는 기술입니다.[5] 이는 데이터 시각화를 용이하게 하거나, 후속 학습 단계의 계산 효율성을 높이기 위한 전처리 과정으로 자주 사용됩니다. 주성분 분석(PCA)이나 오토인코더(Autoencoder)가 대표적인 알고리즘입니다.[5, 10]

#### 본질적 한계

비지도 학습의 결과는 정답이 없기 때문에 지도 학습만큼 명확하게 평가하기 어렵습니다. 발견된 패턴이나 군집의 유효성을 판단하기 위해 종종 사람의 개입과 해석이 필요합니다.[5]

### 1.3 강화 학습: 상호작용과 보상을 통한 학습

강화 학습(Reinforcement Learning)은 앞선 두 패러다임과 근본적으로 다른 접근 방식을 취합니다. 학습의 주체인 **에이전트(agent)**는 정적인 데이터셋으로부터 배우는 것이 아니라, **환경(environment)**과 지속적으로 상호작용하며 학습합니다.[11, 12, 13] 에이전트는 특정 상태(state)에서 행동(action)을 취하고, 그 결과로 환경으로부터 **보상(reward)**이라는 피드백 신호를 받습니다.[7, 10, 12]

#### 핵심 목표

에이전트의 목표는 단기적인 보상이 아닌, 장기적으로 누적되는 보상의 총합을 최대화하는 것입니다.[4, 7] 이를 위해 에이전트는 수많은 시행착오를 통해 어떤 상황에서 어떤 행동을 하는 것이 가장 유리한지를 학습하며, 이러한 전략을 **정책(policy)**이라고 부릅니다.[7]

#### 주요 특징

강화 학습은 지도 학습처럼 각 행동에 대한 '정답'이 주어지지 않으며, 비지도 학습처럼 아무런 피드백이 없는 것도 아닙니다. 대신, 행동의 좋고 나쁨을 평가하는 희소하고 지연된 신호인 '보상'을 사용합니다. 또한, 학습에 필요한 데이터를 에이전트가 스스로의 행동을 통해 능동적으로 생성하고 수집한다는 점에서 다른 패러다임과 뚜렷하게 구분됩니다.[11, 12]

### 1.4 비교 개요 및 새로운 패러다임

세 가지 주요 학습 패러다임의 차이점은 문제 정의 방식과 데이터 요구사항에서 명확하게 드러납니다. 학습 패러다임의 선택은 해결하고자 하는 문제의 성격과 가용한 데이터의 형태에 따라 결정되며, 어느 하나가 절대적으로 우월한 것은 아닙니다.[1, 5]

| 특성 | 지도 학습 (Supervised Learning) | 비지도 학습 (Unsupervised Learning) | 강화 학습 (Reinforcement Learning) |
| :--- | :--- | :--- | :--- |
| **입력 데이터** | 레이블(정답)이 있는 데이터 [2, 4] | 레이블이 없는 데이터 [2, 7] | 정해진 데이터셋 없음, 환경과의 상호작용을 통해 동적으로 생성 [11, 12] |
| **학습 목표** | 새로운 데이터에 대한 정확한 예측 (분류, 회귀) [5, 7] | 데이터 내의 숨겨진 패턴이나 구조 발견 (군집화, 차원 축소) [5, 7] | 누적 보상을 최대화하는 최적의 행동 정책 학습 [4, 7] |
| **피드백** | 명시적인 정답 레이블 [8] | 피드백 없음 [8] | 보상(Reward)이라는 스칼라 신호 [7, 12] |
| **주요 활용 분야** | 스팸 탐지, 이미지 분류, 주가 예측 [5, 6] | 고객 세분화, 추천 시스템, 이상 탐지 [5, 6] | 게임 AI(알파고), 로봇 제어, 자율 주행 [4, 6] |

최근 AI 연구는 이러한 기본적인 패러다임들을 결합하거나 확장하는 방향으로 나아가고 있습니다.

*   **준지도 학습(Semi-Supervised Learning):** 소량의 레이블된 데이터와 대량의 레이블되지 않은 데이터를 함께 사용하여 학습 효율을 높이는 하이브리드 방식입니다.[9, 14]
*   **자기지도 학습(Self-Supervised Learning):** 레이블이 없는 데이터 자체에서 감독 신호(supervisory signal)를 만들어 학습하는 비지도 학습의 한 형태입니다. 예를 들어, 문장의 일부를 가리고 주변 단어를 통해 가려진 단어를 예측하도록 학습하는 방식이 있습니다.[14]
*   **인간 피드백 기반 강화 학습(RLHF):** 인간의 선호도나 피드백을 보상 신호로 변환하여 강화 학습 에이전트를 훈련시키는 방식입니다. 이는 복잡하고 정량화하기 어려운 목표(예: 유용하고 무해한 대화 생성)를 가진 대규모 언어 모델(LLM)을 인간의 가치에 부합하도록 조정하는 데 결정적인 역할을 했습니다.[7, 14]
*   **전이 학습 및 미세 조정(Transfer Learning & Fine-tuning):** 대규모 데이터셋으로 사전 학습된 모델(pre-trained model)을 특정 도메인이나 작업에 맞게 소량의 데이터로 추가 학습(미세 조정)하는 방식입니다. 이는 학습 시간과 데이터 요구량을 획기적으로 줄여줍니다.[15]
*   **검색 증강 생성(RAG):** 생성형 AI 모델이 답변을 생성할 때, 외부 지식 데이터베이스에서 관련 정보를 실시간으로 검색하여 그 내용을 기반으로 보다 정확하고 사실에 근거한 답변을 생성하도록 하는 기술입니다. 이는 모델이 학습하지 않은 최신 정보에 대응하고 환각 현상(Hallucination)을 줄이는 데 효과적입니다.[15]

이러한 패러다임의 진화는 AI가 해결할 수 있는 문제의 범위를 확장하고 있습니다. 특히 지도 학습의 '정답 데이터' 의존성 문제와 강화 학습의 '보상 함수 설계'의 어려움을 해결하기 위해 RLHF와 같은 하이브리드 접근법이 등장한 것은 주목할 만합니다. 이는 한 패러다임 내의 알고리즘 개선을 넘어, 여러 패러다임의 장점을 융합하여 더 복잡한 문제를 해결하려는 AI 발전의 중요한 흐름을 보여줍니다.

---
